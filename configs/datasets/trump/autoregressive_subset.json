{
  "model": {
    "type": "transformer",
    "d_model": 256,
    "num_heads": 4,
    "num_layers": 3,
    "d_ff": 1024,
    "vocab_size": 16000
  },
  "computation": {
    "mode": "pretraining",
    "method": "autoregressive",
    "description": "Wiki subset - Sequential reading (100 files for rapid iteration)"
  },
  "training": {
    "learning_rate": 0.0001,
    "max_length": 128,
    "batch_size": 20,
    "max_tokens": 5000000,
    "num_epochs": 1,
    "shuffle": false
  },
  "data": {
    "input_file": "data/pretraining/wiki/wiki_subset_corpus.txt",
    "output_dir": "outputs/wiki_subset",
    "tokenizer_vocab": "outputs/tokenizer_wiki.vocab"
  },
  "checkpointing": {
    "save_every_n_batches": 250,
    "keep_last_n": 3,
    "save_path": "outputs/wiki_subset/checkpoints"
  }
}
