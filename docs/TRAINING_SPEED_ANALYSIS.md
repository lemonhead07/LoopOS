# Training Speed Analysis & Solutions

## Problem: Slow Training with Large Model

### Observed Performance
- **Small model** (256 dim, 2 layers, 10k vocab): **~330-360 tokens/sec**
- **Large model** (512 dim, 6 layers, 50k vocab): **~48 tokens/sec**

### Root Causes

#### 1. Model Size Impact
The large model is significantly more expensive:

| Component | Small Model | Large Model | Ratio |
|-----------|-------------|-------------|-------|
| Embedding params | 2.56M (10k√ó256) | 25.6M (50k√ó512) | **10x** |
| Transformer layers | 2 layers | 6 layers | **3x** |
| Hidden dimension | 256 | 512 | **2x** |
| Attention complexity | O(n¬≤√ó256) | O(n¬≤√ó512) | **2x** |

**Combined effect**: ~60x slower per forward pass (10√ó3√ó2)

#### 2. Sequence Length Variability
From tokenization analysis:
- **Average sequence**: 88 tokens
- **Max sequence**: 659 tokens  
- **Long sequences** (4,263 out of 7,440) exceed 128 tokens

**Impact**: 
- Attention is O(n¬≤) with sequence length
- 659-token sequence is **(659/128)¬≤ = 26x slower** than 128-token sequence!

#### 3. Previous Bug: Duplicate Forward Pass
**FIXED**: Was doing forward pass twice per training step
- **Before fix**: ~21 tokens/sec
- **After fix**: ~330 tokens/sec
- **Improvement**: 15-16x speedup

## Solutions Implemented

### ‚úÖ 1. Eliminated Duplicate Forward Pass
**Change**: Compute loss directly from existing logits instead of re-running forward pass.

```cpp
// OLD (slow):
auto logits = model_->forward(inputs, inputs);  // First forward
metrics.loss = compute_loss_silent(inputs, targets);  // Second forward inside!

// NEW (fast):
auto logits = model_->forward(inputs, inputs);  // Only forward
// Compute loss directly from logits (no second forward)
```

**Result**: 15x faster

### ‚úÖ 2. Sequence Chunking
**Change**: Split long sequences into max_length chunks.

**Example**:
- Input: 659-token paragraph
- Output: 5 chunks of 128 + 1 chunk of 35 tokens

**Benefits**:
- Predictable compute cost per sequence
- Better hardware utilization
- Prevents outlier sequences from dominating training time

**Statistics**:
- Original: 7,440 sequences
- Chunked: 9,619 sequences (4,263 were split)
- All sequences now ‚â§ 128 tokens

### ‚úÖ 3. Enhanced Logging
Added detailed performance metrics:
- Tokenization statistics (min/max/avg sequence length)
- Per-sample timing every 100 steps
- Epoch timing breakdown (forward %, loss %, overhead %)

## Performance Analysis

### Small Model Performance
```
Model: 256 dim, 2 layers, 10k vocab
Speed: ~330-360 tokens/sec
Time per epoch (9,619 sequences): ~3 minutes
```

### Large Model Expected Performance
```
Model: 512 dim, 6 layers, 50k vocab
Expected speed: ~5-10 tokens/sec (60x slower due to size)
Time per epoch (9,619 sequences): ~3-6 hours
```

**Why is it slow?**
1. **Embedding lookups**: 50k vocab √ó 512 dim = 25M parameters just for embeddings
2. **Attention**: 6 layers √ó O(n¬≤ √ó 512) per sequence
3. **Feed-forward**: 6 layers √ó (512 ‚Üí 2048 ‚Üí 512) transformations

## Recommendations for Faster Training

### üöÄ Short-term (Easy Wins)

#### 1. Use Smaller Vocab
**Current**: 50,000 tokens  
**Recommended**: 10,000-16,000 tokens

**Impact**: 3-5x faster embedding lookups  
**Trade-off**: Less vocabulary coverage (may affect quality)

#### 2. Reduce Model Size for Experimentation
**Current**: 512 dim, 6 layers  
**Recommended**: 256-384 dim, 3-4 layers

**Impact**: 10-20x faster training  
**Trade-off**: Smaller model capacity (use for prototyping)

#### 3. Shorter Sequences
**Current**: max_length=128  
**Recommended**: max_length=64

**Impact**: 4x faster (attention is O(n¬≤))  
**Trade-off**: Less context per training example

### üéØ Medium-term (Better Performance)

#### 4. True Batching
**Current**: Process one sequence at a time  
**Recommended**: Process 32-64 sequences in parallel

**Impact**: 10-30x faster (GPU utilization)  
**Implementation**: Requires padding and batch matrix operations

#### 5. Gradient Accumulation
**Benefit**: Effective larger batch size without memory cost  
**Implementation**: Accumulate gradients over N steps before updating

#### 6. Mixed Precision Training (FP16)
**Benefit**: 2x faster compute, 2x less memory  
**Requirement**: Modern CPU/GPU with FP16 support

### üî¨ Long-term (Major Optimizations)

#### 7. Flash Attention
**Benefit**: 2-4x faster attention computation  
**Complexity**: Requires specialized implementation

#### 8. Gradient Checkpointing
**Benefit**: Trade compute for memory (enables larger models)  
**Use case**: When memory-limited

#### 9. Distributed Training
**Benefit**: Linear speedup with GPUs/nodes  
**Requirement**: Multiple GPUs or machines

## Current Configuration Recommendations

### For Fast Prototyping
```json
{
  "model": {
    "d_model": 256,
    "num_layers": 2,
    "vocab_size": 10000
  },
  "training": {
    "max_length": 64,
    "batch_size": 32,
    "num_epochs": 1
  }
}
```
**Speed**: ~400+ tokens/sec  
**Time per epoch**: ~2 minutes

### For Quality Training
```json
{
  "model": {
    "d_model": 384,
    "num_layers": 4,
    "vocab_size": 16000
  },
  "training": {
    "max_length": 128,
    "batch_size": 32,
    "num_epochs": 10
  }
}
```
**Speed**: ~100-150 tokens/sec  
**Time per epoch**: ~10-15 minutes

### For Production (with optimizations)
```json
{
  "model": {
    "d_model": 512,
    "num_layers": 6,
    "vocab_size": 32000
  },
  "training": {
    "max_length": 128,
    "batch_size": 64,
    "num_epochs": 10
  }
}
```
**With batching**: ~1000+ tokens/sec  
**Time per epoch**: ~2-3 minutes

## Summary

**Current State**:
- ‚úÖ Fixed duplicate forward pass (15x speedup)
- ‚úÖ Added sequence chunking
- ‚úÖ Enhanced performance logging
- ‚úÖ Small model: ~330-360 tokens/sec
- ‚ö†Ô∏è  Large model: ~48 tokens/sec (expected given size)

**Next Steps** (ordered by impact):
1. Reduce vocab size to 16k (5x speedup)
2. Implement true batching (10-30x speedup)
3. Reduce model size for experiments (10x speedup)
4. Add gradient accumulation
5. Explore mixed precision training

**Bottom Line**: The current performance is actually correct given the model size. A 60x larger model should be ~60x slower. To get faster training with the large model, you need batching and/or better hardware (GPU).
