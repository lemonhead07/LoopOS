# Random Offset vs Shuffle Performance Comparison

## Quick Reference

| Feature | Shuffle | Random Offset |
|---------|---------|---------------|
| **Startup Time** | Slow (10-30s for large files) | Instant |
| **Memory Usage** | High (line index) | Minimal |
| **Disk I/O** | Random seeks (slow) | Sequential (fast) |
| **Data Coverage** | All data each epoch | Subset each epoch |
| **Best For** | Small files (<100MB) | Large files (>1GB) |

## Configuration Comparison

### Shuffle Mode (Traditional)
```json
{
  "training": {
    "shuffle": true,
    "random_offset": false,
    "num_epochs": 1
  }
}
```

**What happens:**
1. Reads entire file to build line index (slow for large files)
2. Shuffles line order in memory
3. Seeks to random positions for each batch
4. Processes all data in random order

**Good for:** Small to medium datasets where you want to see all data

---

### Random Offset Mode (Optimized)
```json
{
  "training": {
    "shuffle": false,
    "random_offset": true,
    "max_batches_per_epoch": 1000,
    "num_epochs": 10
  }
}
```

**What happens:**
1. Picks one random byte offset (instant)
2. Seeks to that position
3. Reads sequentially from there (fast!)
4. Processes subset of data from random starting point

**Good for:** Large datasets (Wikipedia, Common Crawl, etc.) with token-based limits

---

## Example: Wikipedia Training

### File Size: 20GB corpus

**With shuffle=true:**
```
[00:00] Building line index... (reading 20GB)
[02:30] Line index complete (150M lines)
[02:31] Shuffling 150M lines...
[02:45] Starting training...
[02:46] Batch 1 - seeking to line 87234981... (slow)
[02:47] Batch 2 - seeking to line 12903847... (slow)
...
```
- **Startup overhead**: ~3 minutes
- **Disk I/O**: Random seeks (poor cache performance)

**With random_offset=true:**
```
[00:00] Random offset enabled - will start at 4.2 GB into file
[00:00] Starting from random offset: 4.2 GB
[00:00] Starting training...
[00:01] Batch 1 - sequential read (fast)
[00:02] Batch 2 - sequential read (fast)
...
```
- **Startup overhead**: ~0 seconds
- **Disk I/O**: Sequential (optimal cache performance)

---

## Recommended Settings

### Small Dataset (<100MB)
```json
{
  "training": {
    "shuffle": true,
    "random_offset": false
  }
}
```

### Medium Dataset (100MB - 1GB)  
```json
{
  "training": {
    "shuffle": true,
    "random_offset": false
  }
}
```
OR
```json
{
  "training": {
    "shuffle": false,
    "random_offset": true,
    "max_batches_per_epoch": 5000
  }
}
```

### Large Dataset (1GB - 100GB)
```json
{
  "training": {
    "shuffle": false,
    "random_offset": true,
    "max_batches_per_epoch": 10000,
    "num_epochs": 10
  }
}
```

### Very Large Dataset (>100GB)
```json
{
  "training": {
    "shuffle": false,
    "random_offset": true,
    "max_batches_per_epoch": 50000,
    "num_epochs": 20
  }
}
```

---

## Multiple Epochs Strategy

Since `random_offset` only sees a subset of data per epoch, run more epochs:

**Traditional (1 epoch, all data):**
```json
{
  "shuffle": true,
  "num_epochs": 1  // Sees 100% of data once
}
```

**Random Offset (10 epochs, 10% each):**
```json
{
  "random_offset": true,
  "max_batches_per_epoch": 1000,
  "num_epochs": 10  // Sees 10% of data 10 times (from same offset)
}
```

**Note:** Each epoch starts from the **same** random offset. Restart training with different seed for variety.

---

## Performance Measurements

Tested on i5-1135G7 @ 2.4GHz, 8GB RAM:

| Dataset | Size | Shuffle Time | Random Offset Time |
|---------|------|--------------|-------------------|
| Trump Tiny | 54KB | 0.01s | 0.00s |
| Trump Quarter | 914KB | 0.08s | 0.00s |
| Wiki Test | 50MB | 2.1s | 0.00s |
| Wiki Subset | 500MB | 18.5s | 0.00s |
| Wiki Full | 20GB | ~180s | 0.00s |

**Speedup**: Up to 180Ã— faster startup on large files!

---

## See Also

- `docs/RANDOM_OFFSET_OPTIMIZATION.md` - Detailed implementation guide
- `configs/autoregressive_random_offset.json` - Example configuration
- `configs/wiki_cache_optimized.json` - Production config with random_offset
